{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.initializers import glorot_uniform\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 48000 # 48kHz\n",
    "DATASET_PATH = \"../../ABA-Audio-Data/Labelled/\"\n",
    "JSON_PATH = \"../../ABA-Audio-Data/data.json\"\n",
    "MELSPECT_OR_MFCC = \"melspect\" # Choose \"melspect\" or \"mfcc\" for train on either data type.\n",
    "N_MFCC = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This structure is used to save and export the data.\n",
    "data = {\n",
    "    # Mapping the different genre lables onto numbers.\n",
    "    \"mapping\": [],\n",
    "    \n",
    "    # Either the MFCC data or the MELSPECT data, depending on mode.\n",
    "    \"all_audio-whine\": [],\n",
    "    \"all_audio-none\": [],\n",
    "    \"all_audio\": [],\n",
    "    \n",
    "    # The targets. Each value in \"labels\" refers to the index of the\n",
    "    # \"labels\" list. \n",
    "    \"labels-whine\": [],\n",
    "    \"labels-none\": [],\n",
    "    \"labels\": []\n",
    "}\n",
    "\n",
    "for i, (dirpath, dirnames, filenames) in enumerate(os.walk(DATASET_PATH)):\n",
    "    if dirpath is not DATASET_PATH:\n",
    "        # This gives us dirpath ~/none and ~/whine.\n",
    "        dirpath_components = dirpath.split('/')\n",
    "        label = dirpath_components[-1]\n",
    "        data[\"mapping\"].append(label)\n",
    "        print(f\"Currently processing label: {label}\")\n",
    "        \n",
    "        for j, file in enumerate(filenames):\n",
    "            # Load audio file.\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "            \n",
    "            # The last second of the video is usually not a full second, and therefore produces\n",
    "            # an nparray that is abnormal in shape to the rest of the arrays. All the arrays\n",
    "            # need to be the same shape, this pads the abnormal arrays with empty data.\n",
    "            if (len(signal) < SAMPLE_RATE):\n",
    "                signal = np.pad(signal, (0, SAMPLE_RATE-len(signal)))\n",
    "                if (signal.shape != (48000,)):\n",
    "                    print(\"Error, bad signal shape.\")\n",
    "            \n",
    "            if (MELSPECT_OR_MFCC == \"melspect\"):\n",
    "                processed_audio = librosa.feature.melspectrogram(y=signal)\n",
    "            elif (MELSPECT_OR_MFCC == \"mfcc\"):\n",
    "                processed_audio = librosa.feature.mfcc(y=signal, sr=SAMPLE_RATE, n_mfcc=N_MFCC)\n",
    "                processed_audio = processed_audio.T\n",
    "            else:\n",
    "                exit('Set MELSPECT_OR_MFCC to either \"melspect\" or \"mfcc\".')\n",
    "            \n",
    "            if (label == \"none\"):\n",
    "                data[\"all_audio-none\"].append(processed_audio.tolist())\n",
    "                data[\"labels-none\"].append(processed_audio.tolist())\n",
    "            elif (label == \"whine\"):\n",
    "                data[\"all_audio-whine\"].append(processed_audio.tolist())\n",
    "                data[\"labels-whine\"].append(processed_audio.tolist())\n",
    "            \n",
    "            data[\"all_audio\"].append(processed_audio.tolist())\n",
    "            data[\"labels\"].append(i-1)\n",
    "            \n",
    "            if (j % 750 == 0):\n",
    "                print(f\"Working file path: {file_path} {j}\")\n",
    "            \n",
    "            final = j\n",
    "        else:\n",
    "            print(final)\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JSON_PATH, \"w\") as fp:\n",
    "    json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the output of this statment for the input of \"input_shape\" in the first\n",
    "# Conv2D layer in the model blow with an added 3rd dimension, like (x, y, 1).\n",
    "print(np.array(data[\"all_audio\"][0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = np.array(data[\"all_audio-none\"])\n",
    "# y1 = np.array(data[\"labels-none\"])\n",
    "\n",
    "# X2 = np.array(data[\"all_audio-whine\"])\n",
    "# y2 = np.array(data[\"labels-whine\"])\n",
    "\n",
    "# print(X1.shape)\n",
    "# print(y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data[\"all_audio\"])\n",
    "y = np.array(data[\"labels\"])\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1, y1 = shuffle(X1, y1)\n",
    "\n",
    "# X1n = X1[:-5594, :]\n",
    "# y1n = y1[:-5594, :]\n",
    "\n",
    "# print(X1n.shape, y1n.shape)\n",
    "# print(X2.shape, y2.shape)\n",
    "\n",
    "# X = np.concatenate((X1n, X2), axis=0)\n",
    "# y = np.concatenate((y1n, y2), axis=0)\n",
    "\n",
    "# X, y = shuffle(X, y)\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test,\n",
    "                                                y_test,\n",
    "                                                test_size=0.10)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 94, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2, 2))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Weights and Biases of model for retraining.\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if hasattr(model.layers[i], 'kernel_initializer') and \\\n",
    "            hasattr(model.layers[i], 'bias_initializer'):\n",
    "        weight_initializer = model.layers[i].kernel_initializer\n",
    "        bias_initializer = model.layers[i].bias_initializer\n",
    "        \n",
    "        old_weights, old_biases = model.layers[i].get_weights()\n",
    "        \n",
    "        model.layers[i].set_weights([\n",
    "            weight_initializer(shape=old_weights.shape),\n",
    "            bias_initializer(shape=old_biases.shape)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.01),\n",
    "              metrics='accuracy')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('whine-cry.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.8, .88)\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da41b803749172fb30e9aa3573affad5b13216880ab231d4dc304fc61cc58ef1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
